{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bdbe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias para o processamento\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# configuração da sessão Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"job-emr-spark\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# definindo o método de logs da aplicação como INFO indicada em ambiente DEV opçoes[INFO,ERROR]\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Carregando os dados do nosso Data Lake\n",
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"True\")\\\n",
    "    .option(\"inferSchema\",\"True\")\\\n",
    "    .csv(\"s3a://landing-jaime/*.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# imprime os dados lidos da LANDING\n",
    "print (\"\\nImprime os dados lidos da lading:\")\n",
    "print (df.show())\n",
    "\n",
    "# imprime o schema do dataframe\n",
    "print (\"\\nImprime o schema do dataframe lido da landing:\")\n",
    "print (df.printSchema())\n",
    "\n",
    "#Limpando os dados\n",
    "df.na.fill(value=0,subset=[\"quantity\"]).show()\n",
    "\n",
    "# converte para formato parquet\n",
    "print (\"\\nEscrevendo os dados lidos da raw para parquet na processing zone...\")\n",
    "df.write.format(\"parquet\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(\"s3a://processing-jaime/df-parquet-file.parquet\")\n",
    "\n",
    "# lendo arquivos parquet\n",
    "df_parquet = spark.read.format(\"parquet\")\\\n",
    " .load(\"s3a://processing-jaime/df-parquet-file.parquet\")\n",
    "\n",
    "# imprime os dados lidos em parquet\n",
    "print (\"\\nImprime os dados lidos em parquet da processing zone\")\n",
    "print (df_parquet.show())\n",
    "\n",
    "# cria uma view para trabalhar com sql\n",
    "df_parquet.createOrReplaceTempView(\"Dados_Sql\")\n",
    "\n",
    "# processa os dados conforme regra de negócio\n",
    "df_sql  = spark.sql(\"SELECT BNF_CODE as Bnf_code \\\n",
    "                       ,SUM(ACT_COST) as Soma_cost \\\n",
    "                       ,SUM(QUANTITY) as Soma_Quantity \\\n",
    "                       ,SUM(ITEMS) as Soma_items \\\n",
    "                       ,AVG(ACT_COST) as Media_cost \\\n",
    "                      FROM Dados_Sql \\\n",
    "                      GROUP BY bnf_code\")\n",
    "\n",
    "\n",
    "# converte para formato parquet\n",
    "print (\"\\nEscrevendo os dados processados na Curated Zone...\")\n",
    "\n",
    "# converte os dados processados para parquet e escreve na curated zone\n",
    "df_sql.write.format(\"parquet\")\\\n",
    "         .mode(\"overwrite\")\\\n",
    "         .save(\"s3a://cureted-jaime/df-result-file.parquet\")\n",
    "\n",
    "# para a aplicação\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
